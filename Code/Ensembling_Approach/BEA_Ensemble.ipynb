{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-04-18T17:53:36.845857Z","iopub.status.busy":"2025-04-18T17:53:36.845556Z","iopub.status.idle":"2025-04-18T17:53:36.950804Z","shell.execute_reply":"2025-04-18T17:53:36.950040Z","shell.execute_reply.started":"2025-04-18T17:53:36.845836Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/numaan.naeem/.conda/envs/ibex/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     /home/numaan.naeem/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import json\n","import torch\n","import random\n","import re\n","import string\n","import nltk\n","import numpy as np\n","import pandas as pd\n","import torch.nn as nn\n","import torch.optim as optim\n","from transformers import AutoTokenizer, AutoModel\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from bs4 import BeautifulSoup\n","import torch\n","from transformers import BertModel, BertTokenizer, XLNetModel, XLNetTokenizer, T5EncoderModel, T5Tokenizer\n","from sklearn.model_selection import train_test_split\n","import re\n","import os\n","import requests\n","import gc\n","from tqdm.auto import tqdm\n","import pandas as pd\n","import numpy as np\n","import joblib\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import train_test_split\n","import joblib\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import train_test_split\n","from transformers import BertTokenizer, BertModel\n","from transformers import RobertaTokenizer, RobertaModel\n","from transformers import GPT2Tokenizer, GPT2Model\n","from transformers import T5Tokenizer, T5EncoderModel\n","from transformers import XLNetTokenizer, XLNetModel\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.metrics import f1_score, accuracy_score\n","\n","from nltk.corpus import stopwords\n","\n","nltk.download('stopwords')\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T15:57:48.691821Z","iopub.status.busy":"2025-04-18T15:57:48.691250Z","iopub.status.idle":"2025-04-18T15:57:48.695270Z","shell.execute_reply":"2025-04-18T15:57:48.694524Z","shell.execute_reply.started":"2025-04-18T15:57:48.691797Z"},"trusted":true},"outputs":[],"source":["train_path = \"/home/numaan.naeem/BEA_2025/mrbench_v3_devset.json\"\n","test_path  = \"/home/numaan.naeem/BEA_2025/mrbench_v3_testset.json\""]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T15:57:55.990016Z","iopub.status.busy":"2025-04-18T15:57:55.989745Z","iopub.status.idle":"2025-04-18T15:57:55.996751Z","shell.execute_reply":"2025-04-18T15:57:55.995837Z","shell.execute_reply.started":"2025-04-18T15:57:55.989995Z"},"trusted":true},"outputs":[],"source":["# -------------------------- Load and preprocess data -------------------------- #\n","def load_and_process_train_data(json_path):\n","    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n","        data = json.load(f)\n","\n","    records = []\n","    for item in data:\n","        conv_id = item[\"conversation_id\"]\n","        history = item[\"conversation_history\"]\n","        for model, details in item[\"tutor_responses\"].items():\n","            record = {\n","                \"conversation_id\": conv_id,\n","                \"model\": model,\n","                \"conversation_history\": history,\n","                \"response\": details[\"response\"],\n","                \"mistake_identification\": details[\"annotation\"][\"Mistake_Identification\"].lower(),\n","            }\n","            records.append(record)\n","\n","    return pd.DataFrame(records)\n","\n","def load_and_process_test_data(json_path):\n","    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n","        data = json.load(f)\n","\n","    records = []\n","    for item in data:\n","        conv_id = item[\"conversation_id\"]\n","        history = item[\"conversation_history\"]\n","        for model, details in item[\"tutor_responses\"].items():\n","            record = {\n","                \"conversation_id\": conv_id,\n","                \"model\": model,\n","                \"conversation_history\": history,\n","                \"response\": details[\"response\"]\n","            }\n","            records.append(record)\n","\n","    return pd.DataFrame(records)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T15:58:02.326230Z","iopub.status.busy":"2025-04-18T15:58:02.325985Z","iopub.status.idle":"2025-04-18T15:58:02.400953Z","shell.execute_reply":"2025-04-18T15:58:02.400320Z","shell.execute_reply.started":"2025-04-18T15:58:02.326213Z"},"trusted":true},"outputs":[],"source":["df_train = load_and_process_train_data(train_path)\n","df_test  = load_and_process_test_data(test_path)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T15:58:07.620194Z","iopub.status.busy":"2025-04-18T15:58:07.619954Z","iopub.status.idle":"2025-04-18T15:58:07.644199Z","shell.execute_reply":"2025-04-18T15:58:07.643633Z","shell.execute_reply.started":"2025-04-18T15:58:07.620177Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>conversation_id</th>\n","      <th>model</th>\n","      <th>conversation_history</th>\n","      <th>response</th>\n","      <th>mistake_identification</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n","      <td>Sonnet</td>\n","      <td>Tutor: Hi, could you please provide a step-by-...</td>\n","      <td>Great, you've correctly identified the cost of...</td>\n","      <td>yes</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n","      <td>Llama318B</td>\n","      <td>Tutor: Hi, could you please provide a step-by-...</td>\n","      <td>Now that we know the cost of 1 pound of meat i...</td>\n","      <td>yes</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n","      <td>Llama31405B</td>\n","      <td>Tutor: Hi, could you please provide a step-by-...</td>\n","      <td>You're close, but I notice that you calculated...</td>\n","      <td>yes</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n","      <td>GPT4</td>\n","      <td>Tutor: Hi, could you please provide a step-by-...</td>\n","      <td>That's correct. So, if 1 pound of meat costs $...</td>\n","      <td>yes</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n","      <td>Mistral</td>\n","      <td>Tutor: Hi, could you please provide a step-by-...</td>\n","      <td>It seems like you've calculated the cost as if...</td>\n","      <td>yes</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n","      <td>Expert</td>\n","      <td>Tutor: Hi, could you please provide a step-by-...</td>\n","      <td>And if there are 5 sandwiches, what is the mea...</td>\n","      <td>yes</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n","      <td>Gemini</td>\n","      <td>Tutor: Hi, could you please provide a step-by-...</td>\n","      <td>You're absolutely right about the cost of meat...</td>\n","      <td>yes</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n","      <td>Phi3</td>\n","      <td>Tutor: Hi, could you please provide a step-by-...</td>\n","      <td>To find the area of a rectangle, multiply its ...</td>\n","      <td>no</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                            conversation_id        model  \\\n","0  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e       Sonnet   \n","1  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e    Llama318B   \n","2  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e  Llama31405B   \n","3  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e         GPT4   \n","4  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e      Mistral   \n","5  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e       Expert   \n","6  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e       Gemini   \n","7  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e         Phi3   \n","\n","                                conversation_history  \\\n","0  Tutor: Hi, could you please provide a step-by-...   \n","1  Tutor: Hi, could you please provide a step-by-...   \n","2  Tutor: Hi, could you please provide a step-by-...   \n","3  Tutor: Hi, could you please provide a step-by-...   \n","4  Tutor: Hi, could you please provide a step-by-...   \n","5  Tutor: Hi, could you please provide a step-by-...   \n","6  Tutor: Hi, could you please provide a step-by-...   \n","7  Tutor: Hi, could you please provide a step-by-...   \n","\n","                                            response mistake_identification  \n","0  Great, you've correctly identified the cost of...                    yes  \n","1  Now that we know the cost of 1 pound of meat i...                    yes  \n","2  You're close, but I notice that you calculated...                    yes  \n","3  That's correct. So, if 1 pound of meat costs $...                    yes  \n","4  It seems like you've calculated the cost as if...                    yes  \n","5  And if there are 5 sandwiches, what is the mea...                    yes  \n","6  You're absolutely right about the cost of meat...                    yes  \n","7  To find the area of a rectangle, multiply its ...                     no  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df_train.head(8)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T15:58:14.279275Z","iopub.status.busy":"2025-04-18T15:58:14.278734Z","iopub.status.idle":"2025-04-18T15:58:14.288873Z","shell.execute_reply":"2025-04-18T15:58:14.287989Z","shell.execute_reply.started":"2025-04-18T15:58:14.279253Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>conversation_id</th>\n","      <th>model</th>\n","      <th>conversation_history</th>\n","      <th>response</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1030-adb61831-0383-4e51-a673-ab978590f69b</td>\n","      <td>Tutor_1</td>\n","      <td>Tutor: Hi, could you please provide a step-by-...</td>\n","      <td>It looks like you've done a great job figuring...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1030-adb61831-0383-4e51-a673-ab978590f69b</td>\n","      <td>Tutor_2</td>\n","      <td>Tutor: Hi, could you please provide a step-by-...</td>\n","      <td>You've done a great job, but there's a small m...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1030-adb61831-0383-4e51-a673-ab978590f69b</td>\n","      <td>Tutor_3</td>\n","      <td>Tutor: Hi, could you please provide a step-by-...</td>\n","      <td>OK, read the question again, and answer these ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1030-adb61831-0383-4e51-a673-ab978590f69b</td>\n","      <td>Tutor_4</td>\n","      <td>Tutor: Hi, could you please provide a step-by-...</td>\n","      <td>Tutor: I see where you're coming from, but I t...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1030-adb61831-0383-4e51-a673-ab978590f69b</td>\n","      <td>Tutor_5</td>\n","      <td>Tutor: Hi, could you please provide a step-by-...</td>\n","      <td>Great job! Can you explain how you arrived at ...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1030-adb61831-0383-4e51-a673-ab978590f69b</td>\n","      <td>Tutor_6</td>\n","      <td>Tutor: Hi, could you please provide a step-by-...</td>\n","      <td>I appreciate your clear explanation, but let's...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1030-adb61831-0383-4e51-a673-ab978590f69b</td>\n","      <td>Tutor_7</td>\n","      <td>Tutor: Hi, could you please provide a step-by-...</td>\n","      <td>It sounds like you're on the right track, but ...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1030-adb61831-0383-4e51-a673-ab978590f69b</td>\n","      <td>Tutor_8</td>\n","      <td>Tutor: Hi, could you please provide a step-by-...</td>\n","      <td>Your method is absolutely correct - multiplyin...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                             conversation_id    model  \\\n","0  1030-adb61831-0383-4e51-a673-ab978590f69b  Tutor_1   \n","1  1030-adb61831-0383-4e51-a673-ab978590f69b  Tutor_2   \n","2  1030-adb61831-0383-4e51-a673-ab978590f69b  Tutor_3   \n","3  1030-adb61831-0383-4e51-a673-ab978590f69b  Tutor_4   \n","4  1030-adb61831-0383-4e51-a673-ab978590f69b  Tutor_5   \n","5  1030-adb61831-0383-4e51-a673-ab978590f69b  Tutor_6   \n","6  1030-adb61831-0383-4e51-a673-ab978590f69b  Tutor_7   \n","7  1030-adb61831-0383-4e51-a673-ab978590f69b  Tutor_8   \n","\n","                                conversation_history  \\\n","0  Tutor: Hi, could you please provide a step-by-...   \n","1  Tutor: Hi, could you please provide a step-by-...   \n","2  Tutor: Hi, could you please provide a step-by-...   \n","3  Tutor: Hi, could you please provide a step-by-...   \n","4  Tutor: Hi, could you please provide a step-by-...   \n","5  Tutor: Hi, could you please provide a step-by-...   \n","6  Tutor: Hi, could you please provide a step-by-...   \n","7  Tutor: Hi, could you please provide a step-by-...   \n","\n","                                            response  \n","0  It looks like you've done a great job figuring...  \n","1  You've done a great job, but there's a small m...  \n","2  OK, read the question again, and answer these ...  \n","3  Tutor: I see where you're coming from, but I t...  \n","4  Great job! Can you explain how you arrived at ...  \n","5  I appreciate your clear explanation, but let's...  \n","6  It sounds like you're on the right track, but ...  \n","7  Your method is absolutely correct - multiplyin...  "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["df_test.head(8)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T15:58:20.888449Z","iopub.status.busy":"2025-04-18T15:58:20.888090Z","iopub.status.idle":"2025-04-18T15:58:20.895158Z","shell.execute_reply":"2025-04-18T15:58:20.894122Z","shell.execute_reply.started":"2025-04-18T15:58:20.888410Z"},"trusted":true},"outputs":[{"data":{"text/plain":["((2476, 5), (1547, 4))"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["df_train.shape, df_test.shape"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T15:58:27.733752Z","iopub.status.busy":"2025-04-18T15:58:27.733074Z","iopub.status.idle":"2025-04-18T15:58:27.744968Z","shell.execute_reply":"2025-04-18T15:58:27.744225Z","shell.execute_reply.started":"2025-04-18T15:58:27.733728Z"},"trusted":true},"outputs":[{"data":{"text/plain":["mistake_identification\n","yes               1932\n","no                 370\n","to some extent     174\n","Name: count, dtype: int64"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["df_train[\"mistake_identification\"].value_counts()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T15:58:34.610067Z","iopub.status.busy":"2025-04-18T15:58:34.609807Z","iopub.status.idle":"2025-04-18T15:58:38.040616Z","shell.execute_reply":"2025-04-18T15:58:38.040028Z","shell.execute_reply.started":"2025-04-18T15:58:34.610046Z"},"trusted":true},"outputs":[],"source":["def clean_text(text):\n","    '''Clean emoji, Make text lowercase, remove text in square brackets,remove links,remove punctuation\n","    and remove words containing numbers.'''\n","    text = re.sub(r'\\:(.*?)\\:','',text)\n","    text = str(text).lower()    #Making Text Lowercase\n","    text = re.sub('\\[.*?\\]', '', text)\n","    #The next 2 lines remove html text\n","    text = BeautifulSoup(text, 'lxml').get_text()\n","    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n","    text = re.sub('<.*?>+', '', text)\n","    text = re.sub('\\n', '', text)\n","    text = re.sub('\\w*\\d\\w*', '', text)\n","    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\", \"'\")\n","    text = re.sub(r\"[^a-zA-Z?.!,¿']+\", \" \", text)\n","    return text\n","\n","\n","def clean_contractions(text):\n","    '''Clean contraction using contraction mapping'''    \n","    specials = [\"’\", \"‘\", \"´\", \"`\"]\n","    for s in specials:\n","        text = text.replace(s, \"'\")\n","    #Remove Punctuations\n","    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n","    # creating a space between a word and the punctuation following it\n","    # eg: \"he is a boy.\" => \"he is a boy .\"\n","    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n","    text = re.sub(r'[\" \"]+', \" \", text)\n","    return text\n","\n","def remove_space(text):\n","    '''Removes awkward spaces'''   \n","    #Removes awkward spaces \n","    text = text.strip()\n","    text = text.split()\n","    return \" \".join(text)\n","\n","def text_preprocessing_pipeline(text):\n","    '''Cleaning and parsing the text.'''\n","    text = clean_text(text)\n","    text = clean_contractions(text)\n","    text = remove_space(text)\n","    # text = remove_stopwords(text)  # Added stopword removal step\n","    return text\n","\n","def remove_stopwords(text):\n","    '''Removes stopwords from the text.'''\n","    stop_words = set(stopwords.words('english'))\n","    text = text.split()\n","    text = [word for word in text if word not in stop_words]\n","    return \" \".join(text)\n","\n","# Training data preprocessing\n","df_train['conversation_history_processed'] = df_train['conversation_history'].apply(text_preprocessing_pipeline)\n","df_train['response_processed'] = df_train['response'].apply(text_preprocessing_pipeline)\n","\n","df_train['conversation_history_processed'] = df_train['conversation_history_processed'].apply(remove_stopwords)\n","df_train['response_processed'] = df_train['response_processed'].apply(remove_stopwords)\n","\n","# Test data preprocessing\n","df_test['conversation_history_processed'] = df_test['conversation_history'].apply(text_preprocessing_pipeline)\n","df_test['response_processed'] = df_test['response'].apply(text_preprocessing_pipeline)\n","\n","df_test['conversation_history_processed'] = df_test['conversation_history_processed'].apply(remove_stopwords)\n","df_test['response_processed'] = df_test['response_processed'].apply(remove_stopwords)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T15:58:46.072885Z","iopub.status.busy":"2025-04-18T15:58:46.072197Z","iopub.status.idle":"2025-04-18T15:58:46.076362Z","shell.execute_reply":"2025-04-18T15:58:46.075481Z","shell.execute_reply.started":"2025-04-18T15:58:46.072861Z"},"trusted":true},"outputs":[],"source":["label_map = {'no': 0, 'yes': 1, 'to some extent': 2}"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T15:58:50.096810Z","iopub.status.busy":"2025-04-18T15:58:50.096037Z","iopub.status.idle":"2025-04-18T15:58:50.110626Z","shell.execute_reply":"2025-04-18T15:58:50.109871Z","shell.execute_reply.started":"2025-04-18T15:58:50.096777Z"},"trusted":true},"outputs":[],"source":["df_train['mistake_identification'] = df_train['mistake_identification'].str.strip().str.lower().map(label_map)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T15:58:55.201469Z","iopub.status.busy":"2025-04-18T15:58:55.200823Z","iopub.status.idle":"2025-04-18T15:58:55.207619Z","shell.execute_reply":"2025-04-18T15:58:55.206710Z","shell.execute_reply.started":"2025-04-18T15:58:55.201445Z"},"trusted":true},"outputs":[],"source":["X_train = df_train.drop(columns=[\"mistake_identification\"])\n","y_train = df_train[\"mistake_identification\"]"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T15:59:01.022927Z","iopub.status.busy":"2025-04-18T15:59:01.022647Z","iopub.status.idle":"2025-04-18T15:59:01.027915Z","shell.execute_reply":"2025-04-18T15:59:01.027280Z","shell.execute_reply.started":"2025-04-18T15:59:01.022905Z"},"trusted":true},"outputs":[{"data":{"text/plain":["((2476, 6), (2476,))"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["X_train.shape, y_train.shape"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T15:59:06.115364Z","iopub.status.busy":"2025-04-18T15:59:06.114768Z","iopub.status.idle":"2025-04-18T15:59:06.119745Z","shell.execute_reply":"2025-04-18T15:59:06.119004Z","shell.execute_reply.started":"2025-04-18T15:59:06.115343Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(2476, 7)"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["df_train.shape"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T15:59:14.062400Z","iopub.status.busy":"2025-04-18T15:59:14.062142Z","iopub.status.idle":"2025-04-18T15:59:14.071282Z","shell.execute_reply":"2025-04-18T15:59:14.070690Z","shell.execute_reply.started":"2025-04-18T15:59:14.062380Z"},"trusted":true},"outputs":[{"data":{"text/plain":["((1980, 6), (496, 6), (1547, 6), (1980,), (496,))"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n","X_train.shape, X_val.shape, df_test.shape, y_train.shape, y_val.shape"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T15:59:20.978710Z","iopub.status.busy":"2025-04-18T15:59:20.978146Z","iopub.status.idle":"2025-04-18T15:59:20.991859Z","shell.execute_reply":"2025-04-18T15:59:20.991034Z","shell.execute_reply.started":"2025-04-18T15:59:20.978687Z"},"trusted":true},"outputs":[],"source":["class LM_EMBED:\n","\n","    def __init__(self, language_model, max_len):\n","        self.lang_model = language_model\n","        self.max_len = max_len\n","\n","        # Import tokenizer and model based on the specified language model\n","        if self.lang_model == 'BERT':\n","            self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","            self.model = BertModel.from_pretrained('bert-base-uncased')\n","        elif self.lang_model == 'ROBERTA':\n","            self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","            self.model = RobertaModel.from_pretrained('roberta-base')\n","        elif self.lang_model == 'GPT2':\n","            self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","            self.model = GPT2Model.from_pretrained('gpt2')\n","            # Assign a padding token if not already present\n","            self.tokenizer.pad_token = self.tokenizer.eos_token\n","            self.model.config.pad_token_id = self.tokenizer.pad_token_id\n","        elif self.lang_model == 'T5':\n","            self.tokenizer = T5Tokenizer.from_pretrained('t5-base')\n","            self.model = T5EncoderModel.from_pretrained('t5-base')\n","        elif self.lang_model == 'XLNET':\n","            self.tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n","            self.model = XLNetModel.from_pretrained('xlnet-base-cased')\n","        else:\n","            raise ValueError(f\"Unsupported language model: {self.lang_model}\")\n","\n","    def extract_word_embs(self, text_df, filename, col_name):\n","        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","        print(f\"Using device: {device}\")\n","\n","        self.model = self.model.to(device)\n","        self.model.eval()\n","\n","        texts = text_df[col_name].tolist()\n","\n","        # Tokenize and encode the texts\n","        tokens = self.tokenizer.batch_encode_plus(\n","            texts,\n","            add_special_tokens=True,\n","            padding='max_length',\n","            max_length=self.max_len,\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","        input_ids = tokens['input_ids'].to(device)\n","        attention_mask = tokens['attention_mask'].to(device)\n","\n","        torch.cuda.empty_cache()\n","\n","        for i in range(0, len(input_ids), 10):\n","            if i % 100 == 0:\n","                print(f\"Processing batch ending at {i}\")\n","            with torch.no_grad():\n","                embeddings = self.model(input_ids=input_ids[i:i+10],\n","                                        attention_mask=attention_mask[i:i+10])[0]\n","                emb_array = embeddings.cpu().numpy()\n","\n","                if i == 0:\n","                    embedding_res = emb_array\n","                else:\n","                    embedding_res = np.concatenate((embedding_res, emb_array))\n","\n","        features = self.extract_features(embedding_res, attention_mask)\n","        padded_arr = self.pad(features)\n","\n","        print(\"Saving embeddings...\")\n","        np.save(filename, padded_arr)\n","\n","    def extract_features(self, emb_res, att_msk):\n","        features = []\n","        for seq_num in range(len(emb_res)):\n","            seq_len = (att_msk[seq_num] == 1).sum()\n","\n","            if self.lang_model in ['BERT', 'ROBERTA']:\n","                seq_emb = emb_res[seq_num][1:seq_len-1]\n","            elif self.lang_model == 'T5':\n","                seq_emb = emb_res[seq_num][:seq_len-1]\n","            elif self.lang_model == 'XLNET':\n","                padded_seq_len = len(att_msk[seq_num])\n","                seq_emb = emb_res[seq_num][padded_seq_len-seq_len:padded_seq_len-2]\n","            elif self.lang_model == 'GPT2':\n","                seq_emb = emb_res[seq_num][:seq_len]\n","            else:\n","                raise ValueError(f\"Unsupported language model: {self.lang_model}\")\n","\n","            features.append(seq_emb)\n","        return np.array(features, dtype=object)\n","\n","    def pad(self, features):\n","        dim1 = self.max_len\n","        dim2 = features[0].shape[1]\n","\n","        for i, feature in enumerate(features):\n","            if i % 100 == 0:\n","                print(f\"Padding batch {i}\")\n","\n","            padded_feature = np.zeros((dim1, dim2))\n","            padded_feature[:feature.shape[0], :feature.shape[1]] = feature\n","\n","            if i == 0:\n","                padded_arr = padded_feature\n","            elif i == 1:\n","                padded_arr = np.stack((padded_arr, padded_feature), axis=0)\n","            else:\n","                reshaped_feature = padded_feature.reshape(1, dim1, dim2)\n","                padded_arr = np.vstack((padded_arr, reshaped_feature))\n","\n","        return padded_arr"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T15:59:26.580899Z","iopub.status.busy":"2025-04-18T15:59:26.580270Z","iopub.status.idle":"2025-04-18T15:59:30.127750Z","shell.execute_reply":"2025-04-18T15:59:30.127232Z","shell.execute_reply.started":"2025-04-18T15:59:26.580855Z"},"trusted":true},"outputs":[],"source":["max_seq_len = 399\n","\n","BERT = LM_EMBED('BERT', max_seq_len)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T16:00:18.865143Z","iopub.status.busy":"2025-04-18T16:00:18.864853Z","iopub.status.idle":"2025-04-18T17:39:56.582480Z","shell.execute_reply":"2025-04-18T17:39:56.581551Z","shell.execute_reply.started":"2025-04-18T16:00:18.865122Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda:0\n","Processing batch ending at 0\n","Processing batch ending at 100\n","Processing batch ending at 200\n","Processing batch ending at 300\n","Processing batch ending at 400\n","Processing batch ending at 500\n","Processing batch ending at 600\n","Processing batch ending at 700\n","Processing batch ending at 800\n","Processing batch ending at 900\n","Processing batch ending at 1000\n","Processing batch ending at 1100\n","Processing batch ending at 1200\n","Processing batch ending at 1300\n","Processing batch ending at 1400\n","Processing batch ending at 1500\n","Processing batch ending at 1600\n","Processing batch ending at 1700\n","Processing batch ending at 1800\n","Processing batch ending at 1900\n","Padding batch 0\n","Padding batch 100\n","Padding batch 200\n","Padding batch 300\n","Padding batch 400\n","Padding batch 500\n","Padding batch 600\n","Padding batch 700\n","Padding batch 800\n","Padding batch 900\n","Padding batch 1000\n","Padding batch 1100\n","Padding batch 1200\n","Padding batch 1300\n","Padding batch 1400\n","Padding batch 1500\n","Padding batch 1600\n","Padding batch 1700\n","Padding batch 1800\n","Padding batch 1900\n","Saving embeddings...\n","Using device: cuda:0\n","Processing batch ending at 0\n","Processing batch ending at 100\n","Processing batch ending at 200\n","Processing batch ending at 300\n","Processing batch ending at 400\n","Padding batch 0\n","Padding batch 100\n","Padding batch 200\n","Padding batch 300\n","Padding batch 400\n","Saving embeddings...\n","Using device: cuda:0\n","Processing batch ending at 0\n","Processing batch ending at 100\n","Processing batch ending at 200\n","Processing batch ending at 300\n","Processing batch ending at 400\n","Processing batch ending at 500\n","Processing batch ending at 600\n","Processing batch ending at 700\n","Processing batch ending at 800\n","Processing batch ending at 900\n","Processing batch ending at 1000\n","Processing batch ending at 1100\n","Processing batch ending at 1200\n","Processing batch ending at 1300\n","Processing batch ending at 1400\n","Processing batch ending at 1500\n","Padding batch 0\n","Padding batch 100\n","Padding batch 200\n","Padding batch 300\n","Padding batch 400\n","Padding batch 500\n","Padding batch 600\n","Padding batch 700\n","Padding batch 800\n","Padding batch 900\n","Padding batch 1000\n","Padding batch 1100\n","Padding batch 1200\n","Padding batch 1300\n","Padding batch 1400\n","Padding batch 1500\n","Saving embeddings...\n","Using device: cuda:0\n","Processing batch ending at 0\n","Processing batch ending at 100\n","Processing batch ending at 200\n","Processing batch ending at 300\n","Processing batch ending at 400\n","Processing batch ending at 500\n","Processing batch ending at 600\n","Processing batch ending at 700\n","Processing batch ending at 800\n","Processing batch ending at 900\n","Processing batch ending at 1000\n","Processing batch ending at 1100\n","Processing batch ending at 1200\n","Processing batch ending at 1300\n","Processing batch ending at 1400\n","Processing batch ending at 1500\n","Processing batch ending at 1600\n","Processing batch ending at 1700\n","Processing batch ending at 1800\n","Processing batch ending at 1900\n","Padding batch 0\n","Padding batch 100\n","Padding batch 200\n","Padding batch 300\n","Padding batch 400\n","Padding batch 500\n","Padding batch 600\n","Padding batch 700\n","Padding batch 800\n","Padding batch 900\n","Padding batch 1000\n","Padding batch 1100\n","Padding batch 1200\n","Padding batch 1300\n","Padding batch 1400\n","Padding batch 1500\n","Padding batch 1600\n","Padding batch 1700\n","Padding batch 1800\n","Padding batch 1900\n","Saving embeddings...\n","Using device: cuda:0\n","Processing batch ending at 0\n","Processing batch ending at 100\n","Processing batch ending at 200\n","Processing batch ending at 300\n","Processing batch ending at 400\n","Padding batch 0\n","Padding batch 100\n","Padding batch 200\n","Padding batch 300\n","Padding batch 400\n","Saving embeddings...\n","Using device: cuda:0\n","Processing batch ending at 0\n","Processing batch ending at 100\n","Processing batch ending at 200\n","Processing batch ending at 300\n","Processing batch ending at 400\n","Processing batch ending at 500\n","Processing batch ending at 600\n","Processing batch ending at 700\n","Processing batch ending at 800\n","Processing batch ending at 900\n","Processing batch ending at 1000\n","Processing batch ending at 1100\n","Processing batch ending at 1200\n","Processing batch ending at 1300\n","Processing batch ending at 1400\n","Processing batch ending at 1500\n","Padding batch 0\n","Padding batch 100\n","Padding batch 200\n","Padding batch 300\n","Padding batch 400\n","Padding batch 500\n","Padding batch 600\n","Padding batch 700\n","Padding batch 800\n","Padding batch 900\n","Padding batch 1000\n","Padding batch 1100\n","Padding batch 1200\n","Padding batch 1300\n","Padding batch 1400\n","Padding batch 1500\n","Saving embeddings...\n"]}],"source":["BERT.extract_word_embs(X_train, \"BERT_EMBED_X_TRAIN_conv.npy\", \"conversation_history_processed\")\n","BERT.extract_word_embs(X_val, \"BERT_EMBED_X_VAL_conv.npy\", \"conversation_history_processed\")\n","BERT.extract_word_embs(df_test, \"BERT_EMBED_X_TEST_conv.npy\", \"conversation_history_processed\")\n","\n","BERT.extract_word_embs(X_train, \"BERT_EMBED_X_TRAIN_resp.npy\", \"response_processed\")\n","BERT.extract_word_embs(X_val, \"BERT_EMBED_X_VAL_resp.npy\", \"response_processed\")\n","BERT.extract_word_embs(df_test, \"BERT_EMBED_X_TEST_resp.npy\", \"response_processed\")"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T17:43:50.792814Z","iopub.status.busy":"2025-04-18T17:43:50.792230Z","iopub.status.idle":"2025-04-18T17:43:50.797264Z","shell.execute_reply":"2025-04-18T17:43:50.796427Z","shell.execute_reply.started":"2025-04-18T17:43:50.792789Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-04-20 22:00:26.543295: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1745172026.992102 1086914 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1745172027.006864 1086914 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1745172027.168858 1086914 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1745172027.168874 1086914 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1745172027.168876 1086914 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1745172027.168878 1086914 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-04-20 22:00:27.174894: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]}],"source":["import os\n","import re\n","import json\n","import string\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from keras.models import Sequential\n","from keras.layers import Embedding, Dense, Flatten, Dropout, InputLayer, Conv2D, MaxPooling2D, BatchNormalization, Input\n","import pandas as pd\n","from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n","import matplotlib.ticker as mtick\n","from matplotlib import pyplot as plt"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T17:44:08.891576Z","iopub.status.busy":"2025-04-18T17:44:08.891039Z","iopub.status.idle":"2025-04-18T17:44:08.896488Z","shell.execute_reply":"2025-04-18T17:44:08.895784Z","shell.execute_reply.started":"2025-04-18T17:44:08.891552Z"},"trusted":true},"outputs":[],"source":["def load_X_data(lang_model):\n","  if lang_model =='BERT':\n","    X_train_conv = np.load('BERT_EMBED_X_TRAIN_conv.npy')\n","    X_val_conv = np.load('BERT_EMBED_X_VAL_conv.npy')\n","    X_train_resp = np.load('BERT_EMBED_X_TRAIN_resp.npy')\n","    X_val_resp = np.load('BERT_EMBED_X_VAL_resp.npy')\n","  elif lang_model =='ROBERTA':\n","    X_train = np.load('ROBERTA_EMBED_X_TRAIN.npy')\n","    X_val = np.load('ROBERTA_EMBED_X_VAL.npy')\n","  elif lang_model =='T5':\n","    X_train = np.load('T5_EMBED_X_TRAIN.npy')\n","    X_val = np.load('T5_EMBED_X_VAL.npy')\n","  elif lang_model =='GPT2':\n","    X_train = np.load('T5XL_BFD_EMBED_X_TRAIN.npy')\n","    X_val = np.load('T5XL_BFD_EMBED_X_VAL.npy')\n","  elif lang_model =='XLNET':\n","    X_train = np.load('XLNET_EMMBED_X_TRAIN.npy')\n","    X_val = np.load('XLNET_EMMBED_X_VAL.npy')\n","\n","  return X_train_conv, X_train_resp, X_val_conv, X_val_resp"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T17:44:09.249697Z","iopub.status.busy":"2025-04-18T17:44:09.249417Z","iopub.status.idle":"2025-04-18T17:44:09.255317Z","shell.execute_reply":"2025-04-18T17:44:09.254502Z","shell.execute_reply.started":"2025-04-18T17:44:09.249676Z"},"trusted":true},"outputs":[],"source":["def load_X_Test_data(lang_model):\n","  if lang_model =='BERT':\n","    print(\"bert\")\n","    X_test_conv = np.load('BERT_EMBED_X_TEST_conv.npy')\n","    X_test_resp = np.load('BERT_EMBED_X_TEST_resp.npy')\n","  elif lang_model =='ROBERTA':\n","    print(\"here\")\n","    X_test = np.load('ROBERTA_EMBED_X_TEST.npy')\n","  elif lang_model =='T5':\n","    print(\"here\")\n","    X_test = np.load('T5_EMBED_X_TEST.npy')\n","  elif lang_model =='GPT2':\n","    X_test = np.load('T5_VELTRI_X_TEST_INDEP.npy')\n","  elif lang_model =='T5XL_BFD':\n","    print(\"T5XL_BFD\")\n","    X_test = np.load('T5XL_BFD_EMBED_X_TEST.npy')\n","  elif lang_model =='XLNET':\n","    print(\"here\")\n","    X_test = np.load('XLNET_EMMBED_X_TEST.npy')\n","  elif lang_model =='VELTRI':\n","    X_test = np.load('Datasets/VELTRI/X_test_VELTRI_Conv.npy')\n","  \n","  \n","  return X_test_conv, X_test_resp"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T17:44:12.112942Z","iopub.status.busy":"2025-04-18T17:44:12.112681Z","iopub.status.idle":"2025-04-18T17:45:37.364482Z","shell.execute_reply":"2025-04-18T17:45:37.362571Z","shell.execute_reply.started":"2025-04-18T17:44:12.112921Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["bert\n"]}],"source":["X_train_conv, X_train_resp, X_val_conv, X_val_resp = load_X_data('BERT')\n","X_test_conv, X_test_resp = load_X_Test_data('BERT')"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T17:45:43.443381Z","iopub.status.busy":"2025-04-18T17:45:43.442661Z","iopub.status.idle":"2025-04-18T17:45:46.464838Z","shell.execute_reply":"2025-04-18T17:45:46.464163Z","shell.execute_reply.started":"2025-04-18T17:45:43.443357Z"},"trusted":true},"outputs":[],"source":["def average_pool_embeddings(embeddings):\n","    \"\"\"\n","    Applies average pooling to reduce the shape of embeddings from (N, L, D) to (N, D).\n","    \n","    Parameters:\n","        embeddings (numpy.ndarray): The input tensor of shape (N, L, D), where\n","                                     N is the number of samples,\n","                                     L is the max sequence length,\n","                                     D is the embedding dimension.\n","    \n","    Returns:\n","        numpy.ndarray: The output tensor of shape (N, D).\n","    \"\"\"\n","    if len(embeddings.shape) != 3:\n","        raise ValueError(\"Input embeddings must have a shape of (N, L, D).\")\n","    \n","    # Average pooling along the sequence length axis\n","    pooled_embeddings = np.mean(embeddings, axis=1)\n","    return pooled_embeddings\n","\n","X_train_conv = average_pool_embeddings(X_train_conv)\n","X_val_conv = average_pool_embeddings(X_val_conv)\n","X_test_conv = average_pool_embeddings(X_test_conv)\n","\n","X_train_resp = average_pool_embeddings(X_train_resp)\n","X_val_resp = average_pool_embeddings(X_val_resp)\n","X_test_resp = average_pool_embeddings(X_test_resp)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T17:47:23.798090Z","iopub.status.busy":"2025-04-18T17:47:23.797543Z","iopub.status.idle":"2025-04-18T17:47:23.974887Z","shell.execute_reply":"2025-04-18T17:47:23.974274Z","shell.execute_reply.started":"2025-04-18T17:47:23.798065Z"},"trusted":true},"outputs":[],"source":["# Final averaged features\n","X_train = (X_train_conv + X_train_resp) / 2\n","X_val = (X_val_conv + X_val_resp) / 2\n","X_test = (X_test_conv + X_test_resp) / 2"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T17:47:33.666311Z","iopub.status.busy":"2025-04-18T17:47:33.665648Z","iopub.status.idle":"2025-04-18T17:47:33.671447Z","shell.execute_reply":"2025-04-18T17:47:33.670793Z","shell.execute_reply.started":"2025-04-18T17:47:33.666289Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train shape: (1980, 768)\n","X_val shape: (496, 768)\n","X_test shape: (1547, 768)\n"]}],"source":["print(\"X_train shape:\", X_train.shape)\n","print(\"X_val shape:\", X_val.shape)\n","print(\"X_test shape:\", X_test.shape)"]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T18:08:24.798532Z","iopub.status.busy":"2025-04-18T18:08:24.797993Z","iopub.status.idle":"2025-04-18T18:08:24.813215Z","shell.execute_reply":"2025-04-18T18:08:24.812627Z","shell.execute_reply.started":"2025-04-18T18:08:24.798485Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import math\n","\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import GradientBoostingClassifier, ExtraTreesRegressor\n","from sklearn.model_selection import LeaveOneOut, cross_validate, cross_val_score, StratifiedShuffleSplit, train_test_split, GridSearchCV, RandomizedSearchCV\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import confusion_matrix, roc_auc_score\n","from sklearn.model_selection import KFold\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.model_selection import GridSearchCV, LeaveOneOut\n","from sklearn.metrics import confusion_matrix\n","import math\n","from sklearn.svm import SVC\n","from sklearn.tree import DecisionTreeClassifier  # Import Decision Tree\n","from sklearn.model_selection import GridSearchCV, LeaveOneOut  # Model selection tools\n","\n","import joblib\n","import warnings\n","\n","# Suppress all warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T18:08:27.388176Z","iopub.status.busy":"2025-04-18T18:08:27.387914Z","iopub.status.idle":"2025-04-18T18:08:27.393328Z","shell.execute_reply":"2025-04-18T18:08:27.392457Z","shell.execute_reply.started":"2025-04-18T18:08:27.388156Z"},"trusted":true},"outputs":[],"source":["def SVM(X,Y,x,y):\n","    param_grid = {\n","        'C': [0.01, 0.1, 1, 10, 100],  # Regularization parameter\n","        'kernel': ['linear', 'rbf', 'sigmoid'],  # Removed 'precomputed' kernel\n","    }\n","    svm=SVC(random_state=42, probability=True)\n","    svm_cv = GridSearchCV(svm, param_grid, scoring='accuracy', cv=KFold(10), n_jobs=4)\n","    \n","    # fitting the model for grid search\n","    svm_cv.fit(X,Y)\n","    svm=svm_cv.best_estimator_\n","    \n","    print('\\n Test Results SVM\\n')\n","    y_p=svm.predict(x)\n","    \n","    print(\"SVM:\\n\", classification_report(y, y_p))\n","    joblib.dump(svm, \"svm_model.joblib\")\n","    \n","    return svm"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T18:08:27.553834Z","iopub.status.busy":"2025-04-18T18:08:27.553271Z","iopub.status.idle":"2025-04-18T18:08:27.558149Z","shell.execute_reply":"2025-04-18T18:08:27.557387Z","shell.execute_reply.started":"2025-04-18T18:08:27.553814Z"},"trusted":true},"outputs":[],"source":["def Tree(X,Y,x,y):\n","    #create new a tree model\n","    DT=DecisionTreeClassifier(random_state=42)\n","    param_grid = {'max_depth':range(2,20),\n","                'criterion':['gini', 'entropy']}\n","    tree_cv = GridSearchCV(DT, param_grid, scoring='accuracy', cv=KFold(10), n_jobs=4)\n","    tree_cv.fit(X, Y)\n","    tree=tree_cv.best_estimator_\n","    \n","    print('\\n Test Results Decision Tree\\n')\n","    y_p=tree.predict(x)\n","\n","    print(\"Tree:\\n\", classification_report(y, y_p))\n","    joblib.dump(tree, \"tree_model.joblib\")\n","    \n","    return tree"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T18:08:27.702524Z","iopub.status.busy":"2025-04-18T18:08:27.702114Z","iopub.status.idle":"2025-04-18T18:08:27.706702Z","shell.execute_reply":"2025-04-18T18:08:27.706026Z","shell.execute_reply.started":"2025-04-18T18:08:27.702486Z"},"trusted":true},"outputs":[],"source":["def NaiveBayes(X, Y, x, y):\n","    # Create a Naive Bayes model\n","    NB = GaussianNB()\n","    \n","    # Define hyperparameter grid (only 'var_smoothing' for GaussianNB)\n","    param_grid = {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]}\n","\n","    # GridSearchCV with Leave-One-Out Cross-Validation\n","    nb_cv = GridSearchCV(NB, param_grid, scoring='accuracy', cv=KFold(10), n_jobs=4)\n","    nb_cv.fit(X, Y)\n","    nb=nb_cv.best_estimator_\n","    \n","    print('\\n Test Results MLP\\n')\n","    y_p=nb.predict(x)\n","\n","    print(\"NB:\\n\", classification_report(y, y_p))\n","    joblib.dump(nb, \"nb_model.joblib\")\n","    \n","    return nb"]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T18:08:27.846450Z","iopub.status.busy":"2025-04-18T18:08:27.846036Z","iopub.status.idle":"2025-04-18T18:08:27.851230Z","shell.execute_reply":"2025-04-18T18:08:27.850406Z","shell.execute_reply.started":"2025-04-18T18:08:27.846429Z"},"trusted":true},"outputs":[],"source":["def KNN(X,Y,x,y):\n","    #create new a knn model\n","    knn = KNeighborsClassifier()\n","    #create a dictionary of all values we want to test for n_neighbors\n","    param_grid = {'n_neighbors': np.arange(1, 10),\n","                'metric': ['minkowski', 'euclidean', 'manhattan']}\n","    #use gridsearch to test all values for n_neighbors\n","    knn_cv = GridSearchCV(knn, param_grid, scoring='accuracy', cv=KFold(10), n_jobs=4)\n","    #fit model to data\n","    knn_cv.fit(X, Y)\n","    knn = knn_cv.best_estimator_\n","    \n","    print('\\n Test Results KNN\\n')\n","    y_p=knn.predict(x)\n","    \n","    print(\"NB:\\n\", classification_report(y, y_p))\n","    joblib.dump(knn, \"knn_model.joblib\")\n","    \n","    return knn"]},{"cell_type":"code","execution_count":63,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T18:08:27.986711Z","iopub.status.busy":"2025-04-18T18:08:27.985968Z","iopub.status.idle":"2025-04-18T18:08:27.991003Z","shell.execute_reply":"2025-04-18T18:08:27.990452Z","shell.execute_reply.started":"2025-04-18T18:08:27.986686Z"},"trusted":true},"outputs":[],"source":["def LRegression(X,Y,x,y):\n","    #create new a tree model\n","    LR=LogisticRegression(random_state=42, max_iter=1500)\n","    param_grid = {'penalty':['l2', None]}\n","    \n","    lr_cv = GridSearchCV(LR, param_grid, scoring='accuracy', cv=KFold(10), n_jobs=4)\n","    lr_cv.fit(X, Y)\n","    lr=lr_cv.best_estimator_\n","    \n","    print('\\n Test Results LR\\n')\n","    y_p=lr.predict(x)\n","    \n","    print(\"NB:\\n\", classification_report(y, y_p))\n","    joblib.dump(lr, \"lr_model.joblib\")\n","    \n","    return lr"]},{"cell_type":"code","execution_count":64,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T18:08:30.357873Z","iopub.status.busy":"2025-04-18T18:08:30.357157Z","iopub.status.idle":"2025-04-18T18:08:30.363040Z","shell.execute_reply":"2025-04-18T18:08:30.362167Z","shell.execute_reply.started":"2025-04-18T18:08:30.357846Z"},"trusted":true},"outputs":[],"source":["def RandomForest(X,Y,x,y):\n","    # Number of trees in random forest\n","    max_depth=range(2,20)\n","    # Number of features to consider at every split\n","    max_features = ['auto', 'sqrt', 'log2']\n","    # Maximum number of levels in tree\n","    criterion = ['gini', 'entropy']\n","    \n","    random_grid = {'max_depth': max_depth,\n","                'criterion': criterion,\n","                'max_features': max_features}\n","    \n","    rf = RandomForestClassifier(random_state=42)\n","    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,\n","                                cv=KFold(10), random_state=42, n_jobs = 4)\n","    rf_random.fit(X, Y)\n","    rf = rf_random.best_estimator_\n","    \n","    print('\\n Test Results RF\\n')\n","    y_p=rf.predict(x)\n","\n","    print(\"RF:\\n\", classification_report(y, y_p))\n","    joblib.dump(rf, \"rf_model.joblib\")\n","    \n","    return rf"]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T18:08:30.504564Z","iopub.status.busy":"2025-04-18T18:08:30.504325Z","iopub.status.idle":"2025-04-18T18:08:30.509549Z","shell.execute_reply":"2025-04-18T18:08:30.508724Z","shell.execute_reply.started":"2025-04-18T18:08:30.504543Z"},"trusted":true},"outputs":[],"source":["def ADABoost(X,Y,x,y):\n","    #create new a tree model\n","    Ada = AdaBoostClassifier(DecisionTreeClassifier(random_state=42), random_state=42)\n","    param_grid = {'estimator__max_depth' : [1, 2, 3, 4, 5],\n","                'estimator__criterion' : ['gini', 'entropy'],\n","                'estimator__splitter' :   ['best', 'random']}\n","    ada_cv = GridSearchCV(Ada, param_grid, cv=KFold(10), scoring = 'accuracy', n_jobs=4)\n","    ada_cv.fit(X,Y)\n","    ada=ada_cv.best_estimator_\n","    \n","    print('\\n Test Results ADABoost\\n')\n","    y_p=ada.predict(x)\n","\n","    print(\"ADABoost:\\n\", classification_report(y, y_p))\n","    joblib.dump(rf, \"ada_model.joblib\")\n","    \n","    return ada"]},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T18:08:30.693351Z","iopub.status.busy":"2025-04-18T18:08:30.692789Z","iopub.status.idle":"2025-04-18T18:08:30.698444Z","shell.execute_reply":"2025-04-18T18:08:30.697694Z","shell.execute_reply.started":"2025-04-18T18:08:30.693334Z"},"trusted":true},"outputs":[],"source":["def MLPClassify(X,Y,x,y):\n","    #create new a tree model\n","    MLP=MLPClassifier(random_state=42, max_iter=1000)\n","    param_grid = {\n","      'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  # Different sizes of hidden layers\n","      'activation': ['relu', 'tanh', 'logistic'],  # Activation functions\n","      'solver': ['adam', 'sgd'],  # Optimization algorithms\n","      'alpha': [0.0001, 0.001, 0.01],  # L2 regularization parameter\n","      'learning_rate': ['constant', 'adaptive']  # Learning rate schedule\n","    }\n","    \n","    mlp_cv = GridSearchCV(MLP, param_grid, scoring='accuracy', cv=KFold(10), n_jobs=4)\n","    mlp_cv.fit(X, Y)\n","    mlp=mlp_cv.best_estimator_\n","    \n","    print('\\n Test Results MLP\\n')\n","    y_p=mlp.predict(x)\n","    \n","    print(\"MLP:\\n\", classification_report(y, y_p))\n","    joblib.dump(rf, \"mlp_model.joblib\")\n","    \n","    return mlp"]},{"cell_type":"code","execution_count":68,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T18:08:44.138599Z","iopub.status.busy":"2025-04-18T18:08:44.138002Z","iopub.status.idle":"2025-04-18T18:12:26.676673Z","shell.execute_reply":"2025-04-18T18:12:26.675684Z","shell.execute_reply.started":"2025-04-18T18:08:44.138569Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"," Test Results SVM\n","\n","SVM:\n","               precision    recall  f1-score   support\n","\n","           0       0.74      0.43      0.55        67\n","           1       0.85      0.98      0.91       396\n","           2       1.00      0.03      0.06        33\n","\n","    accuracy                           0.84       496\n","   macro avg       0.87      0.48      0.51       496\n","weighted avg       0.85      0.84      0.81       496\n","\n"]}],"source":["svm = SVM(X_train, y_train, X_val, y_val)"]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T18:12:26.679624Z","iopub.status.busy":"2025-04-18T18:12:26.678985Z","iopub.status.idle":"2025-04-18T18:15:35.088496Z","shell.execute_reply":"2025-04-18T18:15:35.087722Z","shell.execute_reply.started":"2025-04-18T18:12:26.679596Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"," Test Results Decision Tree\n","\n","Tree:\n","               precision    recall  f1-score   support\n","\n","           0       0.40      0.03      0.06        67\n","           1       0.80      0.99      0.89       396\n","           2       0.00      0.00      0.00        33\n","\n","    accuracy                           0.80       496\n","   macro avg       0.40      0.34      0.31       496\n","weighted avg       0.69      0.80      0.72       496\n","\n"]}],"source":["tree = Tree(X_train, y_train, X_val, y_val)"]},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T18:15:35.089963Z","iopub.status.busy":"2025-04-18T18:15:35.089398Z","iopub.status.idle":"2025-04-18T18:15:36.897696Z","shell.execute_reply":"2025-04-18T18:15:36.896736Z","shell.execute_reply.started":"2025-04-18T18:15:35.089934Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"," Test Results MLP\n","\n","NB:\n","               precision    recall  f1-score   support\n","\n","           0       0.16      0.19      0.18        67\n","           1       0.78      0.55      0.64       396\n","           2       0.09      0.36      0.14        33\n","\n","    accuracy                           0.49       496\n","   macro avg       0.34      0.37      0.32       496\n","weighted avg       0.65      0.49      0.54       496\n","\n"]}],"source":["nb = NaiveBayes(X_train, y_train, X_val, y_val)"]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T18:15:36.900164Z","iopub.status.busy":"2025-04-18T18:15:36.899894Z","iopub.status.idle":"2025-04-18T18:15:49.657202Z","shell.execute_reply":"2025-04-18T18:15:49.656467Z","shell.execute_reply.started":"2025-04-18T18:15:36.900142Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"," Test Results KNN\n","\n","NB:\n","               precision    recall  f1-score   support\n","\n","           0       0.33      0.06      0.10        67\n","           1       0.82      0.99      0.90       396\n","           2       0.50      0.03      0.06        33\n","\n","    accuracy                           0.80       496\n","   macro avg       0.55      0.36      0.35       496\n","weighted avg       0.73      0.80      0.73       496\n","\n"]}],"source":["knn = KNN(X_train, y_train, X_val, y_val)"]},{"cell_type":"code","execution_count":72,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T18:15:49.658048Z","iopub.status.busy":"2025-04-18T18:15:49.657850Z","iopub.status.idle":"2025-04-18T18:16:04.851333Z","shell.execute_reply":"2025-04-18T18:16:04.850712Z","shell.execute_reply.started":"2025-04-18T18:15:49.658032Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"name":"stdout","output_type":"stream","text":["\n"," Test Results LR\n","\n","NB:\n","               precision    recall  f1-score   support\n","\n","           0       0.50      0.01      0.03        67\n","           1       0.80      1.00      0.89       396\n","           2       0.00      0.00      0.00        33\n","\n","    accuracy                           0.80       496\n","   macro avg       0.43      0.34      0.31       496\n","weighted avg       0.71      0.80      0.71       496\n","\n"]}],"source":["lr = LRegression(X_train, y_train, X_val, y_val)"]},{"cell_type":"code","execution_count":73,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T18:16:04.852152Z","iopub.status.busy":"2025-04-18T18:16:04.851928Z","iopub.status.idle":"2025-04-18T18:17:25.789443Z","shell.execute_reply":"2025-04-18T18:17:25.788686Z","shell.execute_reply.started":"2025-04-18T18:16:04.852132Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n"]},{"name":"stdout","output_type":"stream","text":["\n"," Test Results RF\n","\n","RF:\n","               precision    recall  f1-score   support\n","\n","           0       0.67      0.03      0.06        67\n","           1       0.81      1.00      0.89       396\n","           2       1.00      0.06      0.11        33\n","\n","    accuracy                           0.81       496\n","   macro avg       0.82      0.36      0.35       496\n","weighted avg       0.80      0.81      0.73       496\n","\n"]}],"source":["rf = RandomForest(X_train, y_train, X_val, y_val)"]},{"cell_type":"code","execution_count":74,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T18:17:25.790823Z","iopub.status.busy":"2025-04-18T18:17:25.790543Z","iopub.status.idle":"2025-04-18T18:37:48.790566Z","shell.execute_reply":"2025-04-18T18:37:48.789643Z","shell.execute_reply.started":"2025-04-18T18:17:25.790804Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"," Test Results ADABoost\n","\n","ADABoost:\n","               precision    recall  f1-score   support\n","\n","           0       0.48      0.16      0.24        67\n","           1       0.82      0.97      0.89       396\n","           2       0.80      0.12      0.21        33\n","\n","    accuracy                           0.80       496\n","   macro avg       0.70      0.42      0.45       496\n","weighted avg       0.77      0.80      0.76       496\n","\n"]}],"source":["ada = ADABoost(X_train, y_train, X_val, y_val)"]},{"cell_type":"code","execution_count":75,"metadata":{"execution":{"iopub.execute_input":"2025-04-18T18:37:48.792014Z","iopub.status.busy":"2025-04-18T18:37:48.791698Z","iopub.status.idle":"2025-04-18T19:45:31.145823Z","shell.execute_reply":"2025-04-18T19:45:31.144966Z","shell.execute_reply.started":"2025-04-18T18:37:48.791991Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["\n"," Test Results MLP\n","\n","MLP:\n","               precision    recall  f1-score   support\n","\n","           0       0.60      0.39      0.47        67\n","           1       0.85      0.96      0.90       396\n","           2       0.40      0.06      0.11        33\n","\n","    accuracy                           0.82       496\n","   macro avg       0.62      0.47      0.49       496\n","weighted avg       0.79      0.82      0.79       496\n","\n"]}],"source":["mlp = MLPClassify(X_train, y_train, X_val, y_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","meta_train_df = pd.DataFrame({\n","    'svm': svm.predict_proba(X_train)[:, 1],\n","    'knn': knn.predict_proba(X_train)[:, 1],\n","    'nb': nb.predict_proba(X_train)[:, 1],\n","    'tree': tree.predict_proba(X_train)[:, 1],\n","    'rf': rf.predict_proba(X_train)[:, 1],\n","    'ada': ada.predict_proba(X_train)[:, 1],\n","    'mlp': mlp.predict_proba(X_train)[:, 1],\n","    'lr': lr.predict_proba(X_train)[:, 1],\n","})\n","\n","meta_val_df = pd.DataFrame({\n","    'svm': svm.predict_proba(X_val)[:, 1],\n","    'knn': knn.predict_proba(X_val)[:, 1],\n","    'nb': nb.predict_proba(X_val)[:, 1],\n","    'tree': tree.predict_proba(X_val)[:, 1],\n","    'rf': rf.predict_proba(X_val)[:, 1],\n","    'ada': ada.predict_proba(X_val)[:, 1],\n","    'mlp': mlp.predict_proba(X_val)[:, 1],\n","    'lr': lr.predict_proba(X_val)[:, 1],\n","})\n","\n","meta_test_df = pd.DataFrame({\n","    'svm': svm.predict_proba(X_test)[:, 1],\n","    'knn': knn.predict_proba(X_test)[:, 1],\n","    'nb': nb.predict_proba(X_test)[:, 1],\n","    'tree': tree.predict_proba(X_test)[:, 1],\n","    'rf': rf.predict_proba(X_test)[:, 1],\n","    'ada': ada.predict_proba(X_test)[:, 1],\n","    'map': map.predict_proba(X_test)[:, 1],\n","    'lr': lr.predict_proba(X_test)[:, 1],\n","})"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def LRegression(X,Y,x):\n","    #create new a tree model\n","    LR=LogisticRegression(random_state=42, max_iter=1500)\n","    param_grid = {'penalty':['l2', None]}\n","    \n","    lr_cv = GridSearchCV(LR, param_grid, scoring='accuracy', cv=KFold(10), n_jobs=4)\n","    lr_cv.fit(X, Y)\n","    lr=lr_cv.best_estimator_\n","    \n","    print('\\n Test Results LR\\n')\n","    y_p=lr.predict(x)\n","\n","    return lr, yp"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lr, y_p_val = LRegression(meta_train_df, y_train, meta_val_df)\n","print(\"Meta Val:\\n\", classification_report(y_val, y_p_val))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["meta_full_df = pd.concat([meta_train_df, meta_val_df], axis=0).reset_index(drop=True)\n","y_full = np.concatenate([y_train, y_val])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lr, y_p_test = LRegression(meta_full_df, y_full, meta_test_df)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":7183631,"sourceId":11463843,"sourceType":"datasetVersion"}],"dockerImageVersionId":31011,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.21"}},"nbformat":4,"nbformat_minor":4}
