{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/numaan.naeem/.conda/envs/ibex/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/numaan.naeem/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_PATH = \"/home/numaan.naeem/BEA_2025/mrbench_v3_devset.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "records = []\n",
    "\n",
    "for item in data:\n",
    "    conv_id = item[\"conversation_id\"]\n",
    "    history = item[\"conversation_history\"]\n",
    "    for model, details in item[\"tutor_responses\"].items():\n",
    "        record = {\n",
    "            \"conversation_id\": conv_id,\n",
    "            \"model\": model,\n",
    "            \"conversation_history\": history,\n",
    "            \"response\": details[\"response\"],\n",
    "            \"mistake_identification\": details[\"annotation\"][\"Mistake_Identification\"].lower()\n",
    "        }\n",
    "        records.append(record)\n",
    "\n",
    "df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>model</th>\n",
       "      <th>conversation_history</th>\n",
       "      <th>response</th>\n",
       "      <th>mistake_identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>Sonnet</td>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>Great, you've correctly identified the cost of...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>Llama318B</td>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>Now that we know the cost of 1 pound of meat i...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>Llama31405B</td>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>You're close, but I notice that you calculated...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>GPT4</td>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>That's correct. So, if 1 pound of meat costs $...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>Mistral</td>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>It seems like you've calculated the cost as if...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>Expert</td>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>And if there are 5 sandwiches, what is the mea...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>You're absolutely right about the cost of meat...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>Phi3</td>\n",
       "      <td>Tutor: Hi, could you please provide a step-by-...</td>\n",
       "      <td>To find the area of a rectangle, multiply its ...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            conversation_id        model  \\\n",
       "0  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e       Sonnet   \n",
       "1  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e    Llama318B   \n",
       "2  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e  Llama31405B   \n",
       "3  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e         GPT4   \n",
       "4  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e      Mistral   \n",
       "5  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e       Expert   \n",
       "6  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e       Gemini   \n",
       "7  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e         Phi3   \n",
       "\n",
       "                                conversation_history  \\\n",
       "0  Tutor: Hi, could you please provide a step-by-...   \n",
       "1  Tutor: Hi, could you please provide a step-by-...   \n",
       "2  Tutor: Hi, could you please provide a step-by-...   \n",
       "3  Tutor: Hi, could you please provide a step-by-...   \n",
       "4  Tutor: Hi, could you please provide a step-by-...   \n",
       "5  Tutor: Hi, could you please provide a step-by-...   \n",
       "6  Tutor: Hi, could you please provide a step-by-...   \n",
       "7  Tutor: Hi, could you please provide a step-by-...   \n",
       "\n",
       "                                            response mistake_identification  \n",
       "0  Great, you've correctly identified the cost of...                    yes  \n",
       "1  Now that we know the cost of 1 pound of meat i...                    yes  \n",
       "2  You're close, but I notice that you calculated...                    yes  \n",
       "3  That's correct. So, if 1 pound of meat costs $...                    yes  \n",
       "4  It seems like you've calculated the cost as if...                    yes  \n",
       "5  And if there are 5 sandwiches, what is the mea...                    yes  \n",
       "6  You're absolutely right about the cost of meat...                    yes  \n",
       "7  To find the area of a rectangle, multiply its ...                     no  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2476, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mistake_identification\n",
       "yes               1932\n",
       "no                 370\n",
       "to some extent     174\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"mistake_identification\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean emoji, Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = re.sub(r'\\:(.*?)\\:','',text)\n",
    "    text = str(text).lower()    #Making Text Lowercase\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    #The next 2 lines remove html text\n",
    "    text = BeautifulSoup(text, 'lxml').get_text()\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\", \"'\")\n",
    "    text = re.sub(r\"[^a-zA-Z?.!,¿']+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_contractions(text):\n",
    "    '''Clean contraction using contraction mapping'''    \n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    #Remove Punctuations\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
    "    text = re.sub(r'[\" \"]+', \" \", text)\n",
    "    return text\n",
    "\n",
    "def remove_space(text):\n",
    "    '''Removes awkward spaces'''   \n",
    "    #Removes awkward spaces \n",
    "    text = text.strip()\n",
    "    text = text.split()\n",
    "    return \" \".join(text)\n",
    "\n",
    "def text_preprocessing_pipeline(text):\n",
    "    '''Cleaning and parsing the text.'''\n",
    "    text = clean_text(text)\n",
    "    text = clean_contractions(text)\n",
    "    text = remove_space(text)\n",
    "    # text = remove_stopwords(text)  # Added stopword removal step\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    '''Removes stopwords from the text.'''\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = text.split()\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    return \" \".join(text)\n",
    "\n",
    "df['conversation_history'] = df['conversation_history'].apply(text_preprocessing_pipeline)\n",
    "df['response'] = df['response'].apply(text_preprocessing_pipeline)\n",
    "\n",
    "\n",
    "\n",
    "df['conversation_history'] = df['conversation_history'].apply(remove_stopwords)\n",
    "df['response'] = df['response'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>model</th>\n",
       "      <th>conversation_history</th>\n",
       "      <th>response</th>\n",
       "      <th>mistake_identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>Sonnet</td>\n",
       "      <td>tutor tyson decided make muffaletta sandwiches...</td>\n",
       "      <td>great youve correctly identified cost meat let...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>Llama318B</td>\n",
       "      <td>tutor tyson decided make muffaletta sandwiches...</td>\n",
       "      <td>know cost pound meat use information find corr...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>Llama31405B</td>\n",
       "      <td>tutor tyson decided make muffaletta sandwiches...</td>\n",
       "      <td>youre close notice calculated cost pounds meat...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>GPT4</td>\n",
       "      <td>tutor tyson decided make muffaletta sandwiches...</td>\n",
       "      <td>thats correct pound meat costs sandwiches need...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>Mistral</td>\n",
       "      <td>tutor tyson decided make muffaletta sandwiches...</td>\n",
       "      <td>seems like youve calculated cost pounds meat r...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>Expert</td>\n",
       "      <td>tutor tyson decided make muffaletta sandwiches...</td>\n",
       "      <td>sandwiches meat cost</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>tutor tyson decided make muffaletta sandwiches...</td>\n",
       "      <td>youre absolutely right cost meat remember sand...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>Phi3</td>\n",
       "      <td>tutor tyson decided make muffaletta sandwiches...</td>\n",
       "      <td>find area rectangle multiply length width note...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            conversation_id        model  \\\n",
       "0  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e       Sonnet   \n",
       "1  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e    Llama318B   \n",
       "2  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e  Llama31405B   \n",
       "3  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e         GPT4   \n",
       "4  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e      Mistral   \n",
       "5  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e       Expert   \n",
       "6  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e       Gemini   \n",
       "7  221-362eb11a-f190-42a6-b2a4-985fafdcfa9e         Phi3   \n",
       "\n",
       "                                conversation_history  \\\n",
       "0  tutor tyson decided make muffaletta sandwiches...   \n",
       "1  tutor tyson decided make muffaletta sandwiches...   \n",
       "2  tutor tyson decided make muffaletta sandwiches...   \n",
       "3  tutor tyson decided make muffaletta sandwiches...   \n",
       "4  tutor tyson decided make muffaletta sandwiches...   \n",
       "5  tutor tyson decided make muffaletta sandwiches...   \n",
       "6  tutor tyson decided make muffaletta sandwiches...   \n",
       "7  tutor tyson decided make muffaletta sandwiches...   \n",
       "\n",
       "                                            response mistake_identification  \n",
       "0  great youve correctly identified cost meat let...                    yes  \n",
       "1  know cost pound meat use information find corr...                    yes  \n",
       "2  youre close notice calculated cost pounds meat...                    yes  \n",
       "3  thats correct pound meat costs sandwiches need...                    yes  \n",
       "4  seems like youve calculated cost pounds meat r...                    yes  \n",
       "5                               sandwiches meat cost                    yes  \n",
       "6  youre absolutely right cost meat remember sand...                    yes  \n",
       "7  find area rectangle multiply length width note...                     no  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the strings (remove case/space issues)\n",
    "df['mistake_identification'] = df['mistake_identification'].str.strip().str.lower()\n",
    "\n",
    "# Map values\n",
    "label_map = {\n",
    "    'no': 0,\n",
    "    'yes': 1,\n",
    "    'to some extent': 2\n",
    "}\n",
    "\n",
    "df['mistake_identification'] = df['mistake_identification'].map(label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>model</th>\n",
       "      <th>conversation_history</th>\n",
       "      <th>response</th>\n",
       "      <th>mistake_identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>Sonnet</td>\n",
       "      <td>tutor tyson decided make muffaletta sandwiches...</td>\n",
       "      <td>great youve correctly identified cost meat let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>Llama318B</td>\n",
       "      <td>tutor tyson decided make muffaletta sandwiches...</td>\n",
       "      <td>know cost pound meat use information find corr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>Llama31405B</td>\n",
       "      <td>tutor tyson decided make muffaletta sandwiches...</td>\n",
       "      <td>youre close notice calculated cost pounds meat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>GPT4</td>\n",
       "      <td>tutor tyson decided make muffaletta sandwiches...</td>\n",
       "      <td>thats correct pound meat costs sandwiches need...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>221-362eb11a-f190-42a6-b2a4-985fafdcfa9e</td>\n",
       "      <td>Mistral</td>\n",
       "      <td>tutor tyson decided make muffaletta sandwiches...</td>\n",
       "      <td>seems like youve calculated cost pounds meat r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2471</th>\n",
       "      <td>5910-25617a89-a4ae-47bb-8812-d6b39fa4e691</td>\n",
       "      <td>Mistral</td>\n",
       "      <td>tutor hector purchased container gumballs gave...</td>\n",
       "      <td>seems might misunderstanding calculating numbe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2472</th>\n",
       "      <td>5910-25617a89-a4ae-47bb-8812-d6b39fa4e691</td>\n",
       "      <td>Phi3</td>\n",
       "      <td>tutor hector purchased container gumballs gave...</td>\n",
       "      <td>solve problem need add number apples person al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>5910-25617a89-a4ae-47bb-8812-d6b39fa4e691</td>\n",
       "      <td>Sonnet</td>\n",
       "      <td>tutor hector purchased container gumballs gave...</td>\n",
       "      <td>thats great start like worked backwards lets t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>5910-25617a89-a4ae-47bb-8812-d6b39fa4e691</td>\n",
       "      <td>Expert</td>\n",
       "      <td>tutor hector purchased container gumballs gave...</td>\n",
       "      <td>okay hector gave less four times many bobby gi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2475</th>\n",
       "      <td>5910-25617a89-a4ae-47bb-8812-d6b39fa4e691</td>\n",
       "      <td>Llama318B</td>\n",
       "      <td>tutor hector purchased container gumballs gave...</td>\n",
       "      <td>thats good attempt lets examine step calculate...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2476 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                conversation_id        model  \\\n",
       "0      221-362eb11a-f190-42a6-b2a4-985fafdcfa9e       Sonnet   \n",
       "1      221-362eb11a-f190-42a6-b2a4-985fafdcfa9e    Llama318B   \n",
       "2      221-362eb11a-f190-42a6-b2a4-985fafdcfa9e  Llama31405B   \n",
       "3      221-362eb11a-f190-42a6-b2a4-985fafdcfa9e         GPT4   \n",
       "4      221-362eb11a-f190-42a6-b2a4-985fafdcfa9e      Mistral   \n",
       "...                                         ...          ...   \n",
       "2471  5910-25617a89-a4ae-47bb-8812-d6b39fa4e691      Mistral   \n",
       "2472  5910-25617a89-a4ae-47bb-8812-d6b39fa4e691         Phi3   \n",
       "2473  5910-25617a89-a4ae-47bb-8812-d6b39fa4e691       Sonnet   \n",
       "2474  5910-25617a89-a4ae-47bb-8812-d6b39fa4e691       Expert   \n",
       "2475  5910-25617a89-a4ae-47bb-8812-d6b39fa4e691    Llama318B   \n",
       "\n",
       "                                   conversation_history  \\\n",
       "0     tutor tyson decided make muffaletta sandwiches...   \n",
       "1     tutor tyson decided make muffaletta sandwiches...   \n",
       "2     tutor tyson decided make muffaletta sandwiches...   \n",
       "3     tutor tyson decided make muffaletta sandwiches...   \n",
       "4     tutor tyson decided make muffaletta sandwiches...   \n",
       "...                                                 ...   \n",
       "2471  tutor hector purchased container gumballs gave...   \n",
       "2472  tutor hector purchased container gumballs gave...   \n",
       "2473  tutor hector purchased container gumballs gave...   \n",
       "2474  tutor hector purchased container gumballs gave...   \n",
       "2475  tutor hector purchased container gumballs gave...   \n",
       "\n",
       "                                               response  \\\n",
       "0     great youve correctly identified cost meat let...   \n",
       "1     know cost pound meat use information find corr...   \n",
       "2     youre close notice calculated cost pounds meat...   \n",
       "3     thats correct pound meat costs sandwiches need...   \n",
       "4     seems like youve calculated cost pounds meat r...   \n",
       "...                                                 ...   \n",
       "2471  seems might misunderstanding calculating numbe...   \n",
       "2472  solve problem need add number apples person al...   \n",
       "2473  thats great start like worked backwards lets t...   \n",
       "2474  okay hector gave less four times many bobby gi...   \n",
       "2475  thats good attempt lets examine step calculate...   \n",
       "\n",
       "      mistake_identification  \n",
       "0                          1  \n",
       "1                          1  \n",
       "2                          1  \n",
       "3                          1  \n",
       "4                          1  \n",
       "...                      ...  \n",
       "2471                       1  \n",
       "2472                       0  \n",
       "2473                       1  \n",
       "2474                       1  \n",
       "2475                       2  \n",
       "\n",
       "[2476 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mistake_identification\n",
       "1    1932\n",
       "0     370\n",
       "2     174\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"mistake_identification\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(df.drop([\"mistake_identification\"], axis=1), df[\"mistake_identification\"], test_size=0.2, stratify=df[\"mistake_identification\"], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1980, 4), (496, 4))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is:  cuda\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "# MODEL_NAME = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device is: \", DEVICE)\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 10\n",
    "LR = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoryDataset(Dataset):\n",
    "    def __init__(self, samples):\n",
    "        # samples = list of (history_str, response_str, label_int)\n",
    "        self.samples = samples\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = list(zip(\n",
    "    X_train['conversation_history'],\n",
    "    X_train['response'],\n",
    "    y_train\n",
    "))\n",
    "\n",
    "test_samples = list(zip(\n",
    "    X_val['conversation_history'],\n",
    "    X_val['response'],\n",
    "    y_val\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = HistoryDataset(train_samples)\n",
    "test_ds  = HistoryDataset(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetModel(\n",
       "  (embeddings): MPNetEmbeddings(\n",
       "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): MPNetEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (pooler): MPNetPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "encoder   = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "encoder.eval()  # We'll freeze it, as in the paper's approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLS + Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CLS]\n",
    "@torch.no_grad()\n",
    "def get_sequence_embeddings(texts):\n",
    "    \"\"\"\n",
    "    Return shape: [batch_size, hidden_dim]\n",
    "    \"\"\"\n",
    "    enc = tokenizer(texts, return_tensors=\"pt\", padding=True,\n",
    "                    truncation=True, return_attention_mask=True).to(DEVICE)\n",
    "    outputs = encoder(**enc)\n",
    "    # CLS token is at position 0\n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :]  # [batch, hidden_dim]\n",
    "    return cls_embeddings\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: list of (history_str, response_str, label_int)\n",
    "    We'll embed them in a single pass for efficiency.\n",
    "    Returns (hist_emb, resp_emb, labels).\n",
    "    \"\"\"\n",
    "    hist_texts = [item[0] for item in batch]\n",
    "    resp_texts = [item[1] for item in batch]\n",
    "    labels = [item[2] for item in batch]\n",
    "\n",
    "    # shape => [B, hist_len, hidden_dim]\n",
    "    hist_emb = get_sequence_embeddings(hist_texts)\n",
    "    # shape => [B, resp_len, hidden_dim]\n",
    "    resp_emb = get_sequence_embeddings(resp_texts)\n",
    "\n",
    "    labels_t = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return hist_emb, resp_emb, labels_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL WITH [CSL] OR POOLING\n",
    "class SimpleHistoryBasedModel(nn.Module):\n",
    "    def __init__(self, hidden_dim=768, num_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Project both embeddings to a common space (optional but useful)\n",
    "        self.history_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.response_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # You can use element-wise interaction (e.g., concat, diff, dot)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim // 2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, hist_emb, resp_emb):\n",
    "        \"\"\"\n",
    "        hist_emb: [batch, hidden_dim]\n",
    "        resp_emb: [batch, hidden_dim]\n",
    "        Returns logits => [batch, num_classes]\n",
    "        \"\"\"\n",
    "\n",
    "        # Optionally project inputs\n",
    "        hist_proj = self.history_proj(hist_emb)  # [B, H]\n",
    "        resp_proj = self.response_proj(resp_emb)  # [B, H]\n",
    "\n",
    "        # Combine them — here we use concatenation\n",
    "        combined = torch.cat([hist_proj, resp_proj], dim=-1)  # [B, 2*H]\n",
    "\n",
    "        # Feedforward for classification\n",
    "        logits = self.ff(combined)  # [B, num_classes]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleHistoryBasedModel(\n",
    "    hidden_dim=768,\n",
    "    num_classes=3\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5778\n",
      "Epoch 2/10, Train Loss: 0.4313\n",
      "Epoch 3/10, Train Loss: 0.3810\n",
      "Epoch 4/10, Train Loss: 0.3296\n",
      "Epoch 5/10, Train Loss: 0.2973\n",
      "Epoch 6/10, Train Loss: 0.2753\n",
      "Epoch 7/10, Train Loss: 0.2464\n",
      "Epoch 8/10, Train Loss: 0.2222\n",
      "Epoch 9/10, Train Loss: 0.2010\n",
      "Epoch 10/10, Train Loss: 0.1815\n",
      "\n",
      "TEST RESULTS:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           Yes (0)       0.75      0.59      0.66        74\n",
      "            No (1)       0.88      0.92      0.90       387\n",
      "To Some Extent (2)       0.25      0.23      0.24        35\n",
      "\n",
      "          accuracy                           0.82       496\n",
      "         macro avg       0.62      0.58      0.60       496\n",
      "      weighted avg       0.81      0.82      0.82       496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# 5) TRAINING LOOP\n",
    "###############################################################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for hist_emb, resp_emb, labels in train_loader:\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(hist_emb, resp_emb)  # [batch, num_classes]\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {avg_loss:.4f}\")\n",
    "###############################################################################\n",
    "# 6) EVALUATION ON TEST SET\n",
    "###############################################################################\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels= []\n",
    "with torch.no_grad():\n",
    "    for hist_emb, resp_emb, labels in test_loader:\n",
    "        labels = labels.to(DEVICE)\n",
    "        logits = model(hist_emb, resp_emb)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "all_preds  = np.concatenate(all_preds, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "print(\"\\nTEST RESULTS:\")\n",
    "print(classification_report(\n",
    "    all_labels,\n",
    "    all_preds,\n",
    "    target_names=[\"No (0)\", \"Yes (1)\", \"To Some Extent (2)\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEAN POOLING & Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEAN POOLING\n",
    "@torch.no_grad()\n",
    "def get_sequence_embeddings(texts):\n",
    "    enc = tokenizer(texts, return_tensors=\"pt\", padding=True,\n",
    "                    truncation=True, return_attention_mask=True).to(DEVICE)\n",
    "    outputs = encoder(**enc)\n",
    "    token_embeddings = outputs.last_hidden_state  # [batch, seq_len, hidden_dim]\n",
    "    attention_mask = enc['attention_mask'].unsqueeze(-1)  # [batch, seq_len, 1]\n",
    "\n",
    "    # Zero out pad tokens, then average\n",
    "    sum_embeddings = (token_embeddings * attention_mask).sum(1)\n",
    "    valid_token_count = attention_mask.sum(1)\n",
    "    mean_embeddings = sum_embeddings / valid_token_count  # [batch, hidden_dim]\n",
    "    return mean_embeddings\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: list of (history_str, response_str, label_int)\n",
    "    We'll embed them in a single pass for efficiency.\n",
    "    Returns (hist_emb, resp_emb, labels).\n",
    "    \"\"\"\n",
    "    hist_texts = [item[0] for item in batch]\n",
    "    resp_texts = [item[1] for item in batch]\n",
    "    labels = [item[2] for item in batch]\n",
    "\n",
    "    # shape => [B, hist_len, hidden_dim]\n",
    "    hist_emb = get_sequence_embeddings(hist_texts)\n",
    "    # shape => [B, resp_len, hidden_dim]\n",
    "    resp_emb = get_sequence_embeddings(resp_texts)\n",
    "\n",
    "    labels_t = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return hist_emb, resp_emb, labels_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL WITH [CSL] OR POOLING\n",
    "class SimpleHistoryBasedModel(nn.Module):\n",
    "    def __init__(self, hidden_dim=768, num_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Project both embeddings to a common space (optional but useful)\n",
    "        self.history_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.response_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # You can use element-wise interaction (e.g., concat, diff, dot)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim // 2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, hist_emb, resp_emb):\n",
    "        \"\"\"\n",
    "        hist_emb: [batch, hidden_dim]\n",
    "        resp_emb: [batch, hidden_dim]\n",
    "        Returns logits => [batch, num_classes]\n",
    "        \"\"\"\n",
    "\n",
    "        # Optionally project inputs\n",
    "        hist_proj = self.history_proj(hist_emb)  # [B, H]\n",
    "        resp_proj = self.response_proj(resp_emb)  # [B, H]\n",
    "\n",
    "        # Combine them — here we use concatenation\n",
    "        combined = torch.cat([hist_proj, resp_proj], dim=-1)  # [B, 2*H]\n",
    "\n",
    "        # Feedforward for classification\n",
    "        logits = self.ff(combined)  # [B, num_classes]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleHistoryBasedModel(\n",
    "    hidden_dim=768,\n",
    "    num_classes=3\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5747\n",
      "Epoch 2/10, Train Loss: 0.4362\n",
      "Epoch 3/10, Train Loss: 0.3829\n",
      "Epoch 4/10, Train Loss: 0.3425\n",
      "Epoch 5/10, Train Loss: 0.3022\n",
      "Epoch 6/10, Train Loss: 0.2824\n",
      "Epoch 7/10, Train Loss: 0.2628\n",
      "Epoch 8/10, Train Loss: 0.2310\n",
      "Epoch 9/10, Train Loss: 0.2064\n",
      "Epoch 10/10, Train Loss: 0.1870\n",
      "\n",
      "TEST RESULTS:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           Yes (0)       0.74      0.57      0.64        74\n",
      "            No (1)       0.87      0.94      0.90       387\n",
      "To Some Extent (2)       0.30      0.17      0.22        35\n",
      "\n",
      "          accuracy                           0.83       496\n",
      "         macro avg       0.64      0.56      0.59       496\n",
      "      weighted avg       0.81      0.83      0.82       496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# 5) TRAINING LOOP\n",
    "###############################################################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for hist_emb, resp_emb, labels in train_loader:\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(hist_emb, resp_emb)  # [batch, num_classes]\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {avg_loss:.4f}\")\n",
    "###############################################################################\n",
    "# 6) EVALUATION ON TEST SET\n",
    "###############################################################################\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels= []\n",
    "with torch.no_grad():\n",
    "    for hist_emb, resp_emb, labels in test_loader:\n",
    "        labels = labels.to(DEVICE)\n",
    "        logits = model(hist_emb, resp_emb)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "all_preds  = np.concatenate(all_preds, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "print(\"\\nTEST RESULTS:\")\n",
    "print(classification_report(\n",
    "    all_labels,\n",
    "    all_preds,\n",
    "    target_names=[\"No (0)\", \"Yes (1)\", \"To Some Extent (2)\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLS/MEAN & Siamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CLS]\n",
    "@torch.no_grad()\n",
    "def get_sequence_embeddings(texts):\n",
    "    \"\"\"\n",
    "    Return shape: [batch_size, hidden_dim]\n",
    "    \"\"\"\n",
    "    enc = tokenizer(texts, return_tensors=\"pt\", padding=True,\n",
    "                    truncation=True, return_attention_mask=True).to(DEVICE)\n",
    "    outputs = encoder(**enc)\n",
    "    # CLS token is at position 0\n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :]  # [batch, hidden_dim]\n",
    "    return cls_embeddings\n",
    "\n",
    "# # MEAN POOLING\n",
    "# @torch.no_grad()\n",
    "# def get_sequence_embeddings(texts):\n",
    "#     enc = tokenizer(texts, return_tensors=\"pt\", padding=True,\n",
    "#                     truncation=True, return_attention_mask=True).to(DEVICE)\n",
    "#     outputs = encoder(**enc)\n",
    "#     token_embeddings = outputs.last_hidden_state  # [batch, seq_len, hidden_dim]\n",
    "#     attention_mask = enc['attention_mask'].unsqueeze(-1)  # [batch, seq_len, 1]\n",
    "\n",
    "#     # Zero out pad tokens, then average\n",
    "#     sum_embeddings = (token_embeddings * attention_mask).sum(1)\n",
    "#     valid_token_count = attention_mask.sum(1)\n",
    "#     mean_embeddings = sum_embeddings / valid_token_count  # [batch, hidden_dim]\n",
    "#     return mean_embeddings\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: list of (history_str, response_str, label_int)\n",
    "    We'll embed them in a single pass for efficiency.\n",
    "    Returns (hist_emb, resp_emb, labels).\n",
    "    \"\"\"\n",
    "    hist_texts = [item[0] for item in batch]\n",
    "    resp_texts = [item[1] for item in batch]\n",
    "    labels = [item[2] for item in batch]\n",
    "\n",
    "    # shape => [B, hist_len, hidden_dim]\n",
    "    hist_emb = get_sequence_embeddings(hist_texts)\n",
    "    # shape => [B, resp_len, hidden_dim]\n",
    "    resp_emb = get_sequence_embeddings(resp_texts)\n",
    "\n",
    "    labels_t = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return hist_emb, resp_emb, labels_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SiameseWithCosineClassifier(nn.Module):\n",
    "    def __init__(self, hidden_dim=768, num_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Classifier uses h, r, |h - r|, cos_sim\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 3 + 1, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, hist_emb, resp_emb):\n",
    "        # Optional projection layer\n",
    "        h = self.proj(hist_emb)  # [B, 768]\n",
    "        r = self.proj(resp_emb)  # [B, 768]\n",
    "\n",
    "        # Cosine similarity between h and r\n",
    "        cos_sim = F.cosine_similarity(h, r, dim=1)  # [B]\n",
    "        cos_sim = cos_sim.unsqueeze(1)              # [B, 1]\n",
    "\n",
    "        # Concatenate: h, r, |h - r|, cosine sim\n",
    "        combined = torch.cat([\n",
    "            h, r, torch.abs(h - r), cos_sim\n",
    "        ], dim=-1)  # [B, 3*768 + 1]\n",
    "\n",
    "        logits = self.ff(combined)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseWithCosineClassifier(\n",
    "    hidden_dim=768,\n",
    "    num_classes=3\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5959\n",
      "Epoch 2/10, Train Loss: 0.4395\n",
      "Epoch 3/10, Train Loss: 0.3641\n",
      "Epoch 4/10, Train Loss: 0.2876\n",
      "Epoch 5/10, Train Loss: 0.2152\n",
      "Epoch 6/10, Train Loss: 0.1414\n",
      "Epoch 7/10, Train Loss: 0.0809\n",
      "Epoch 8/10, Train Loss: 0.0485\n",
      "Epoch 9/10, Train Loss: 0.0287\n",
      "Epoch 10/10, Train Loss: 0.0251\n",
      "\n",
      "TEST RESULTS:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           Yes (0)       0.73      0.61      0.66        74\n",
      "            No (1)       0.88      0.94      0.91       387\n",
      "To Some Extent (2)       0.26      0.14      0.19        35\n",
      "\n",
      "          accuracy                           0.83       496\n",
      "         macro avg       0.62      0.56      0.58       496\n",
      "      weighted avg       0.81      0.83      0.82       496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# 5) TRAINING LOOP\n",
    "###############################################################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for hist_emb, resp_emb, labels in train_loader:\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(hist_emb, resp_emb)  # [batch, num_classes]\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {avg_loss:.4f}\")\n",
    "###############################################################################\n",
    "# 6) EVALUATION ON TEST SET\n",
    "###############################################################################\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels= []\n",
    "with torch.no_grad():\n",
    "    for hist_emb, resp_emb, labels in test_loader:\n",
    "        labels = labels.to(DEVICE)\n",
    "        logits = model(hist_emb, resp_emb)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "all_preds  = np.concatenate(all_preds, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "print(\"\\nTEST RESULTS:\")\n",
    "print(classification_report(\n",
    "    all_labels,\n",
    "    all_preds,\n",
    "    target_names=[\"No (0)\", \"Yes (1)\", \"To Some Extent (2)\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOKEN LEVEL & KQV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKEN LEVEL EMBEDDING\n",
    "@torch.no_grad()\n",
    "def get_sequence_embeddings(texts):\n",
    "    \"\"\"\n",
    "    texts: list of strings\n",
    "    Return shape: [batch_size, seq_len, hidden_dim]\n",
    "    We do *no pooling*, we keep the full token sequence for attention.\n",
    "    \"\"\"\n",
    "    enc = tokenizer(texts, return_tensors=\"pt\", padding=True,\n",
    "                    truncation=True).to(DEVICE)\n",
    "    outputs = encoder(**enc)\n",
    "    return outputs.last_hidden_state  # [batch, seq_len, hidden_dim]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: list of (history_str, response_str, label_int)\n",
    "    We'll embed them in a single pass for efficiency.\n",
    "    Returns (hist_emb, resp_emb, labels).\n",
    "    \"\"\"\n",
    "    hist_texts = [item[0] for item in batch]\n",
    "    resp_texts = [item[1] for item in batch]\n",
    "    labels = [item[2] for item in batch]\n",
    "\n",
    "    # shape => [B, hist_len, hidden_dim]\n",
    "    hist_emb = get_sequence_embeddings(hist_texts)\n",
    "    # shape => [B, resp_len, hidden_dim]\n",
    "    resp_emb = get_sequence_embeddings(resp_texts)\n",
    "\n",
    "    labels_t = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return hist_emb, resp_emb, labels_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 4) MODEL DEFINITION: SIMPLE HISTORY-BASED\n",
    "###############################################################################\n",
    "# This follows the paper's architecture for the \"Simple History-Based Model\":\n",
    "#  - K from \"previous sentence\" embeddings (the conversation_history)\n",
    "#  - Q,V from \"current sentence\" (the tutor response).\n",
    "#  - MultiHeadAttention (Q=resp, K=hist, V=resp).\n",
    "#  - Then we pool the output and pass it through a small feed-forward to get 3-class logits.\n",
    "\n",
    "class SimpleHistoryBasedModel(nn.Module):\n",
    "    def __init__(self, hidden_dim=768, n_heads=8, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=hidden_dim,\n",
    "                                         num_heads=n_heads,\n",
    "                                         batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LeakyReLU(),  # paper typically used some activation\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, hist_emb, resp_emb):\n",
    "        \"\"\"\n",
    "        hist_emb: [batch, hist_len, hidden_dim] -> used as K\n",
    "        resp_emb: [batch, resp_len, hidden_dim] -> used as Q & V\n",
    "        Returns logits => [batch, num_classes]\n",
    "        \"\"\"\n",
    "        # standard multi-head attention: Q=resp, K=hist, V=resp\n",
    "        # attn_out => [batch, resp_len, hidden_dim]\n",
    "        attn_out, _ = self.mha(query=resp_emb,\n",
    "                               key=hist_emb,\n",
    "                               value=hist_emb)\n",
    "\n",
    "        # We can pool over resp_len dimension to get a single vector\n",
    "        # The paper used a feed-forward on \"the output of the attention mechanism\"\n",
    "        # We'll do a simple mean-pool:\n",
    "        pooled = attn_out.mean(dim=1)  # => [batch, hidden_dim]\n",
    "        logits = self.ff(pooled)       # => [batch, num_classes]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleHistoryBasedModel(\n",
    "    hidden_dim=768,\n",
    "    num_classes=3\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6388\n",
      "Epoch 2/10, Train Loss: 0.4716\n",
      "Epoch 3/10, Train Loss: 0.4111\n",
      "Epoch 4/10, Train Loss: 0.3743\n",
      "Epoch 5/10, Train Loss: 0.3396\n",
      "Epoch 6/10, Train Loss: 0.3046\n",
      "Epoch 7/10, Train Loss: 0.2742\n",
      "Epoch 8/10, Train Loss: 0.2424\n",
      "Epoch 9/10, Train Loss: 0.2216\n",
      "Epoch 10/10, Train Loss: 0.2060\n",
      "\n",
      "TEST RESULTS:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           Yes (0)       0.75      0.66      0.71        74\n",
      "            No (1)       0.88      0.95      0.91       387\n",
      "To Some Extent (2)       0.27      0.09      0.13        35\n",
      "\n",
      "          accuracy                           0.85       496\n",
      "         macro avg       0.63      0.57      0.58       496\n",
      "      weighted avg       0.82      0.85      0.83       496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# 5) TRAINING LOOP\n",
    "###############################################################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for hist_emb, resp_emb, labels in train_loader:\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(hist_emb, resp_emb)  # [batch, num_classes]\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {avg_loss:.4f}\")\n",
    "###############################################################################\n",
    "# 6) EVALUATION ON TEST SET\n",
    "###############################################################################\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels= []\n",
    "with torch.no_grad():\n",
    "    for hist_emb, resp_emb, labels in test_loader:\n",
    "        labels = labels.to(DEVICE)\n",
    "        logits = model(hist_emb, resp_emb)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "all_preds  = np.concatenate(all_preds, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "print(\"\\nTEST RESULTS:\")\n",
    "print(classification_report(\n",
    "    all_labels,\n",
    "    all_preds,\n",
    "    target_names=[\"Yes (0)\", \"No (1)\", \"To Some Extent (2)\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
