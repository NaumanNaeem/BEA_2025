{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/home/numaan.naeem/BEA_2025/mrbench_v3_devset.json\"\n",
    "test_path  = \"/home/numaan.naeem/BEA_2025/mrbench_v3_testset.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- Load and preprocess data -------------------------- #\n",
    "def load_and_process_train_data(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    records = []\n",
    "    for item in data:\n",
    "        conv_id = item[\"conversation_id\"]\n",
    "        history = item[\"conversation_history\"]\n",
    "        for model, details in item[\"tutor_responses\"].items():\n",
    "            record = {\n",
    "                \"conversation_id\": conv_id,\n",
    "                \"model\": model,\n",
    "                \"conversation_history\": history,\n",
    "                \"response\": details[\"response\"],\n",
    "                \"mistake_identification\": details[\"annotation\"][\"Mistake_Identification\"].lower()\n",
    "            }\n",
    "            records.append(record)\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def load_and_process_test_data(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    records = []\n",
    "    for item in data:\n",
    "        conv_id = item[\"conversation_id\"]\n",
    "        history = item[\"conversation_history\"]\n",
    "        for model, details in item[\"tutor_responses\"].items():\n",
    "            record = {\n",
    "                \"conversation_id\": conv_id,\n",
    "                \"model\": model,\n",
    "                \"conversation_history\": history,\n",
    "                \"response\": details[\"response\"]\n",
    "            }\n",
    "            records.append(record)\n",
    "\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = load_and_process_train_data(train_path)\n",
    "df_test  = load_and_process_test_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"mistake_identification\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean emoji, Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = re.sub(r'\\:(.*?)\\:','',text)\n",
    "    text = str(text).lower()    #Making Text Lowercase\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    #The next 2 lines remove html text\n",
    "    text = BeautifulSoup(text, 'lxml').get_text()\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\", \"'\")\n",
    "    text = re.sub(r\"[^a-zA-Z?.!,¿']+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_contractions(text):\n",
    "    '''Clean contraction using contraction mapping'''    \n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    #Remove Punctuations\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
    "    text = re.sub(r'[\" \"]+', \" \", text)\n",
    "    return text\n",
    "\n",
    "def remove_space(text):\n",
    "    '''Removes awkward spaces'''   \n",
    "    #Removes awkward spaces \n",
    "    text = text.strip()\n",
    "    text = text.split()\n",
    "    return \" \".join(text)\n",
    "\n",
    "def text_preprocessing_pipeline(text):\n",
    "    '''Cleaning and parsing the text.'''\n",
    "    text = clean_text(text)\n",
    "    text = clean_contractions(text)\n",
    "    text = remove_space(text)\n",
    "    # text = remove_stopwords(text)  # Added stopword removal step\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    '''Removes stopwords from the text.'''\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = text.split()\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    return \" \".join(text)\n",
    "\n",
    "# for df in [df_train, df_test]:\n",
    "df['conversation_history'] = df['conversation_history'].apply(text_preprocessing_pipeline)\n",
    "df['response'] = df['response'].apply(text_preprocessing_pipeline)\n",
    "df['conversation_history'] = df['conversation_history'].apply(remove_stopwords)\n",
    "df['response'] = df['response'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'No': 0, 'Yes': 1, 'To some extent': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['mistake_identification'] = df_train['mistake_identification'].str.strip().str.lower().map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(columns=[\"mistake_identification\"])\n",
    "y_train = df_train[\"mistake_identification\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "# MODEL_NAME = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device is: \", DEVICE)\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 10\n",
    "LR = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoryDataset(Dataset):\n",
    "    def __init__(self, samples):\n",
    "        # samples = list of (history_str, response_str, label_int)\n",
    "        self.samples = samples\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = list(zip(X_train['conversation_history'], X_train['response'], y_train))\n",
    "test_samples  = list(zip(df_test['conversation_history'], df_test['response'], [0]*len(df_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = HistoryDataset(train_samples)\n",
    "test_ds  = HistoryDataset(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "encoder   = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "encoder.eval()  # We'll freeze it, as in the paper's approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKEN LEVEL EMBEDDING\n",
    "@torch.no_grad()\n",
    "def get_sequence_embeddings(texts):\n",
    "    \"\"\"\n",
    "    texts: list of strings\n",
    "    Return shape: [batch_size, seq_len, hidden_dim]\n",
    "    We do *no pooling*, we keep the full token sequence for attention.\n",
    "    \"\"\"\n",
    "    enc = tokenizer(texts, return_tensors=\"pt\", padding=True,\n",
    "                    truncation=True).to(DEVICE)\n",
    "    outputs = encoder(**enc)\n",
    "    return outputs.last_hidden_state  # [batch, seq_len, hidden_dim]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: list of (history_str, response_str, label_int)\n",
    "    We'll embed them in a single pass for efficiency.\n",
    "    Returns (hist_emb, resp_emb, labels).\n",
    "    \"\"\"\n",
    "    hist_texts = [item[0] for item in batch]\n",
    "    resp_texts = [item[1] for item in batch]\n",
    "    labels = [item[2] for item in batch]\n",
    "\n",
    "    # shape => [B, hist_len, hidden_dim]\n",
    "    hist_emb = get_sequence_embeddings(hist_texts)\n",
    "    # shape => [B, resp_len, hidden_dim]\n",
    "    resp_emb = get_sequence_embeddings(resp_texts)\n",
    "\n",
    "    labels_t = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return hist_emb, resp_emb, labels_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 4) MODEL DEFINITION: SIMPLE HISTORY-BASED\n",
    "###############################################################################\n",
    "# This follows the paper's architecture for the \"Simple History-Based Model\":\n",
    "#  - K from \"previous sentence\" embeddings (the conversation_history)\n",
    "#  - Q,V from \"current sentence\" (the tutor response).\n",
    "#  - MultiHeadAttention (Q=resp, K=hist, V=resp).\n",
    "#  - Then we pool the output and pass it through a small feed-forward to get 3-class logits.\n",
    "\n",
    "class SimpleHistoryBasedModel(nn.Module):\n",
    "    def __init__(self, hidden_dim=768, n_heads=8, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=hidden_dim,\n",
    "                                         num_heads=n_heads,\n",
    "                                         batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LeakyReLU(),  # paper typically used some activation\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, hist_emb, resp_emb):\n",
    "        \"\"\"\n",
    "        hist_emb: [batch, hist_len, hidden_dim] -> used as K\n",
    "        resp_emb: [batch, resp_len, hidden_dim] -> used as Q & V\n",
    "        Returns logits => [batch, num_classes]\n",
    "        \"\"\"\n",
    "        # standard multi-head attention: Q=resp, K=hist, V=resp\n",
    "        # attn_out => [batch, resp_len, hidden_dim]\n",
    "        attn_out, _ = self.mha(query=resp_emb,\n",
    "                               key=hist_emb,\n",
    "                               value=hist_emb)\n",
    "\n",
    "        # We can pool over resp_len dimension to get a single vector\n",
    "        # The paper used a feed-forward on \"the output of the attention mechanism\"\n",
    "        # We'll do a simple mean-pool:\n",
    "        pooled = attn_out.mean(dim=1)  # => [batch, hidden_dim]\n",
    "        logits = self.ff(pooled)       # => [batch, num_classes]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleHistoryBasedModel(\n",
    "    hidden_dim=768,\n",
    "    num_classes=3\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 5) TRAINING LOOP\n",
    "###############################################################################\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for hist_emb, resp_emb, labels in train_loader:\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(hist_emb, resp_emb)  # [batch, num_classes]\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- Prediction on Test Set ----------------------------- #\n",
    "model.eval()\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for hist_emb, resp_emb, _ in test_loader:\n",
    "        logits = model(hist_emb, resp_emb)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        test_preds.append(preds.cpu().numpy())\n",
    "\n",
    "test_preds = np.concatenate(test_preds, axis=0)\n",
    "df_test['predicted_mistake_identification'] = test_preds\n",
    "print(\"Sample Predictions:\")\n",
    "print(df_test[['conversation_id', 'model', 'predicted_mistake_identification']].head())\n",
    "\n",
    "# Save predictions to CSV\n",
    "df_test.to_csv(\"predicted_mistake_identifications_token.csv\", index=False)\n",
    "print(\"Predictions exported to predicted_mistake_identifications.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
